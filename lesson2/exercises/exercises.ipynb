{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this jupyter notebook we will solve the exercises from lesson 2 in Andrej Karpathy's Youtube series on LLM's\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **E01**: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use counting first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And importing PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our characters to integers and integers to charachters dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the  difference begins. We will not use a two dimensional matrix to store the counts. We will use a three dimensional matrix since we are doing a trigram model. The first two coordinates will correspond to the first two letters of a triple of letters. Our context to predict the third letter is now two letters, as opposed to the bigram model, where we were using only 1 letter as context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise our count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the information into the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert our 3D-matrix \"N\" of counts into a probability distribution over the first two dimensions. We also sum 1 to every entry so that every letter is possible to come after other 2, even if it is unlikely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27, 1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = (N+1).float()\n",
    "P.sum(2, keepdim = True).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  27.],\n",
       "         [4437.],\n",
       "         [1333.],\n",
       "         [1569.],\n",
       "         [1717.],\n",
       "         [1558.],\n",
       "         [ 444.],\n",
       "         [ 696.],\n",
       "         [ 901.],\n",
       "         [ 618.],\n",
       "         [2449.],\n",
       "         [2990.],\n",
       "         [1599.],\n",
       "         [2565.],\n",
       "         [1173.],\n",
       "         [ 421.],\n",
       "         [ 542.],\n",
       "         [ 119.],\n",
       "         [1666.],\n",
       "         [2082.],\n",
       "         [1335.],\n",
       "         [ 105.],\n",
       "         [ 403.],\n",
       "         [ 334.],\n",
       "         [ 161.],\n",
       "         [ 562.],\n",
       "         [ 956.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 583.],\n",
       "         [ 568.],\n",
       "         [ 497.],\n",
       "         [1069.],\n",
       "         [ 719.],\n",
       "         [ 161.],\n",
       "         [ 195.],\n",
       "         [2359.],\n",
       "         [1677.],\n",
       "         [ 202.],\n",
       "         [ 595.],\n",
       "         [2555.],\n",
       "         [1661.],\n",
       "         [5465.],\n",
       "         [  90.],\n",
       "         [ 109.],\n",
       "         [  87.],\n",
       "         [3291.],\n",
       "         [1145.],\n",
       "         [ 714.],\n",
       "         [ 408.],\n",
       "         [ 861.],\n",
       "         [ 188.],\n",
       "         [ 209.],\n",
       "         [2077.],\n",
       "         [ 462.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 348.],\n",
       "         [  65.],\n",
       "         [  28.],\n",
       "         [  92.],\n",
       "         [ 682.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  68.],\n",
       "         [ 244.],\n",
       "         [  28.],\n",
       "         [  27.],\n",
       "         [ 130.],\n",
       "         [  27.],\n",
       "         [  31.],\n",
       "         [ 132.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [ 869.],\n",
       "         [  35.],\n",
       "         [  29.],\n",
       "         [  72.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [ 110.],\n",
       "         [  27.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 842.],\n",
       "         [  27.],\n",
       "         [  69.],\n",
       "         [  28.],\n",
       "         [ 578.],\n",
       "         [  27.],\n",
       "         [  29.],\n",
       "         [ 691.],\n",
       "         [ 298.],\n",
       "         [  30.],\n",
       "         [ 343.],\n",
       "         [ 143.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [ 407.],\n",
       "         [  28.],\n",
       "         [  38.],\n",
       "         [ 103.],\n",
       "         [  32.],\n",
       "         [  62.],\n",
       "         [  62.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  30.],\n",
       "         [ 131.],\n",
       "         [  31.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [1330.],\n",
       "         [  28.],\n",
       "         [  30.],\n",
       "         [ 176.],\n",
       "         [1310.],\n",
       "         [  32.],\n",
       "         [  52.],\n",
       "         [ 145.],\n",
       "         [ 701.],\n",
       "         [  36.],\n",
       "         [  30.],\n",
       "         [  87.],\n",
       "         [  57.],\n",
       "         [  58.],\n",
       "         [ 405.],\n",
       "         [  27.],\n",
       "         [  28.],\n",
       "         [ 451.],\n",
       "         [  56.],\n",
       "         [  31.],\n",
       "         [ 119.],\n",
       "         [  44.],\n",
       "         [  50.],\n",
       "         [  27.],\n",
       "         [ 344.],\n",
       "         [  28.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 706.],\n",
       "         [ 148.],\n",
       "         [ 180.],\n",
       "         [ 411.],\n",
       "         [1298.],\n",
       "         [ 109.],\n",
       "         [ 152.],\n",
       "         [ 179.],\n",
       "         [ 845.],\n",
       "         [  82.],\n",
       "         [ 205.],\n",
       "         [3275.],\n",
       "         [ 796.],\n",
       "         [2702.],\n",
       "         [ 296.],\n",
       "         [ 110.],\n",
       "         [  41.],\n",
       "         [1985.],\n",
       "         [ 888.],\n",
       "         [ 607.],\n",
       "         [  96.],\n",
       "         [ 490.],\n",
       "         [  77.],\n",
       "         [ 159.],\n",
       "         [1097.],\n",
       "         [ 208.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 269.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [ 150.],\n",
       "         [  71.],\n",
       "         [  28.],\n",
       "         [  28.],\n",
       "         [ 187.],\n",
       "         [  27.],\n",
       "         [  29.],\n",
       "         [  47.],\n",
       "         [  27.],\n",
       "         [  31.],\n",
       "         [  87.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [ 141.],\n",
       "         [  33.],\n",
       "         [  45.],\n",
       "         [  37.],\n",
       "         [  27.],\n",
       "         [  31.],\n",
       "         [  27.],\n",
       "         [  41.],\n",
       "         [  29.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 357.],\n",
       "         [  30.],\n",
       "         [  27.],\n",
       "         [  46.],\n",
       "         [ 361.],\n",
       "         [  28.],\n",
       "         [  52.],\n",
       "         [ 387.],\n",
       "         [ 217.],\n",
       "         [  30.],\n",
       "         [  27.],\n",
       "         [  59.],\n",
       "         [  33.],\n",
       "         [  54.],\n",
       "         [ 110.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [ 228.],\n",
       "         [  57.],\n",
       "         [  58.],\n",
       "         [ 112.],\n",
       "         [  28.],\n",
       "         [  53.],\n",
       "         [  27.],\n",
       "         [  58.],\n",
       "         [  28.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [2271.],\n",
       "         [  35.],\n",
       "         [  29.],\n",
       "         [  51.],\n",
       "         [ 701.],\n",
       "         [  29.],\n",
       "         [  29.],\n",
       "         [  28.],\n",
       "         [ 756.],\n",
       "         [  36.],\n",
       "         [  56.],\n",
       "         [ 212.],\n",
       "         [ 144.],\n",
       "         [ 165.],\n",
       "         [ 314.],\n",
       "         [  28.],\n",
       "         [  28.],\n",
       "         [ 231.],\n",
       "         [  58.],\n",
       "         [  98.],\n",
       "         [ 193.],\n",
       "         [  66.],\n",
       "         [  37.],\n",
       "         [  27.],\n",
       "         [ 240.],\n",
       "         [  47.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [2472.],\n",
       "         [ 137.],\n",
       "         [ 536.],\n",
       "         [ 467.],\n",
       "         [1680.],\n",
       "         [ 128.],\n",
       "         [ 455.],\n",
       "         [ 122.],\n",
       "         [ 109.],\n",
       "         [ 103.],\n",
       "         [ 472.],\n",
       "         [1372.],\n",
       "         [ 454.],\n",
       "         [2153.],\n",
       "         [ 615.],\n",
       "         [  80.],\n",
       "         [  79.],\n",
       "         [ 876.],\n",
       "         [1343.],\n",
       "         [ 568.],\n",
       "         [ 136.],\n",
       "         [ 296.],\n",
       "         [  35.],\n",
       "         [ 116.],\n",
       "         [ 806.],\n",
       "         [ 304.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [1500.],\n",
       "         [  28.],\n",
       "         [  31.],\n",
       "         [  31.],\n",
       "         [ 467.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  72.],\n",
       "         [ 146.],\n",
       "         [  29.],\n",
       "         [  29.],\n",
       "         [  36.],\n",
       "         [  32.],\n",
       "         [  29.],\n",
       "         [ 506.],\n",
       "         [  28.],\n",
       "         [  27.],\n",
       "         [  38.],\n",
       "         [  34.],\n",
       "         [  29.],\n",
       "         [ 229.],\n",
       "         [  32.],\n",
       "         [  33.],\n",
       "         [  27.],\n",
       "         [  37.],\n",
       "         [  27.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [1758.],\n",
       "         [  29.],\n",
       "         [  29.],\n",
       "         [  29.],\n",
       "         [ 922.],\n",
       "         [  28.],\n",
       "         [  27.],\n",
       "         [ 334.],\n",
       "         [ 536.],\n",
       "         [  29.],\n",
       "         [  47.],\n",
       "         [ 166.],\n",
       "         [  36.],\n",
       "         [  53.],\n",
       "         [ 371.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [ 136.],\n",
       "         [ 122.],\n",
       "         [  44.],\n",
       "         [  77.],\n",
       "         [  29.],\n",
       "         [  61.],\n",
       "         [  27.],\n",
       "         [ 406.],\n",
       "         [  29.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [2650.],\n",
       "         [  79.],\n",
       "         [  52.],\n",
       "         [ 165.],\n",
       "         [2948.],\n",
       "         [  49.],\n",
       "         [  33.],\n",
       "         [  46.],\n",
       "         [2507.],\n",
       "         [  33.],\n",
       "         [  51.],\n",
       "         [1372.],\n",
       "         [  87.],\n",
       "         [  41.],\n",
       "         [ 719.],\n",
       "         [  42.],\n",
       "         [  30.],\n",
       "         [  45.],\n",
       "         [ 121.],\n",
       "         [ 104.],\n",
       "         [ 351.],\n",
       "         [  99.],\n",
       "         [  43.],\n",
       "         [  27.],\n",
       "         [1615.],\n",
       "         [  37.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [2617.],\n",
       "         [ 139.],\n",
       "         [  78.],\n",
       "         [  51.],\n",
       "         [ 845.],\n",
       "         [  28.],\n",
       "         [  27.],\n",
       "         [  32.],\n",
       "         [1283.],\n",
       "         [  34.],\n",
       "         [  28.],\n",
       "         [  32.],\n",
       "         [ 195.],\n",
       "         [  47.],\n",
       "         [ 479.],\n",
       "         [  65.],\n",
       "         [  27.],\n",
       "         [ 124.],\n",
       "         [  62.],\n",
       "         [  31.],\n",
       "         [ 166.],\n",
       "         [  30.],\n",
       "         [  29.],\n",
       "         [  27.],\n",
       "         [ 314.],\n",
       "         [  38.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [3004.],\n",
       "         [  35.],\n",
       "         [ 240.],\n",
       "         [ 731.],\n",
       "         [1386.],\n",
       "         [  38.],\n",
       "         [ 300.],\n",
       "         [  53.],\n",
       "         [1752.],\n",
       "         [  71.],\n",
       "         [  85.],\n",
       "         [ 222.],\n",
       "         [  46.],\n",
       "         [1933.],\n",
       "         [ 523.],\n",
       "         [  32.],\n",
       "         [  29.],\n",
       "         [  71.],\n",
       "         [ 305.],\n",
       "         [ 470.],\n",
       "         [ 123.],\n",
       "         [  82.],\n",
       "         [  38.],\n",
       "         [  33.],\n",
       "         [ 492.],\n",
       "         [ 172.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 176.],\n",
       "         [ 167.],\n",
       "         [ 141.],\n",
       "         [ 217.],\n",
       "         [ 159.],\n",
       "         [  61.],\n",
       "         [  71.],\n",
       "         [ 198.],\n",
       "         [  96.],\n",
       "         [  43.],\n",
       "         [  95.],\n",
       "         [ 646.],\n",
       "         [ 288.],\n",
       "         [2438.],\n",
       "         [ 142.],\n",
       "         [ 122.],\n",
       "         [  30.],\n",
       "         [1086.],\n",
       "         [ 531.],\n",
       "         [ 145.],\n",
       "         [ 302.],\n",
       "         [ 203.],\n",
       "         [ 141.],\n",
       "         [  72.],\n",
       "         [ 130.],\n",
       "         [  81.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 236.],\n",
       "         [  29.],\n",
       "         [  28.],\n",
       "         [  27.],\n",
       "         [ 224.],\n",
       "         [  28.],\n",
       "         [  27.],\n",
       "         [ 231.],\n",
       "         [  88.],\n",
       "         [  28.],\n",
       "         [  28.],\n",
       "         [  43.],\n",
       "         [  28.],\n",
       "         [  28.],\n",
       "         [  86.],\n",
       "         [  66.],\n",
       "         [  27.],\n",
       "         [ 178.],\n",
       "         [  43.],\n",
       "         [  44.],\n",
       "         [  31.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  39.],\n",
       "         [  27.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [  40.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  28.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  40.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  28.],\n",
       "         [  29.],\n",
       "         [  27.],\n",
       "         [  29.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  28.],\n",
       "         [  29.],\n",
       "         [  27.],\n",
       "         [ 233.],\n",
       "         [  27.],\n",
       "         [  30.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  27.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [2383.],\n",
       "         [  68.],\n",
       "         [ 126.],\n",
       "         [ 214.],\n",
       "         [1724.],\n",
       "         [  36.],\n",
       "         [ 103.],\n",
       "         [ 148.],\n",
       "         [3060.],\n",
       "         [  52.],\n",
       "         [ 117.],\n",
       "         [ 440.],\n",
       "         [ 189.],\n",
       "         [ 167.],\n",
       "         [ 896.],\n",
       "         [  41.],\n",
       "         [  43.],\n",
       "         [ 452.],\n",
       "         [ 217.],\n",
       "         [ 235.],\n",
       "         [ 279.],\n",
       "         [ 107.],\n",
       "         [  48.],\n",
       "         [  30.],\n",
       "         [ 800.],\n",
       "         [  50.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [1228.],\n",
       "         [  48.],\n",
       "         [  87.],\n",
       "         [  36.],\n",
       "         [ 911.],\n",
       "         [  29.],\n",
       "         [  29.],\n",
       "         [1312.],\n",
       "         [ 711.],\n",
       "         [  29.],\n",
       "         [ 109.],\n",
       "         [ 306.],\n",
       "         [ 117.],\n",
       "         [  51.],\n",
       "         [ 558.],\n",
       "         [  78.],\n",
       "         [  28.],\n",
       "         [  82.],\n",
       "         [ 488.],\n",
       "         [ 792.],\n",
       "         [ 212.],\n",
       "         [  41.],\n",
       "         [  51.],\n",
       "         [  27.],\n",
       "         [ 242.],\n",
       "         [  37.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [1054.],\n",
       "         [  28.],\n",
       "         [  44.],\n",
       "         [  27.],\n",
       "         [ 743.],\n",
       "         [  29.],\n",
       "         [  29.],\n",
       "         [ 674.],\n",
       "         [ 559.],\n",
       "         [  30.],\n",
       "         [  27.],\n",
       "         [ 161.],\n",
       "         [  31.],\n",
       "         [  49.],\n",
       "         [ 694.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [ 379.],\n",
       "         [  62.],\n",
       "         [ 401.],\n",
       "         [ 105.],\n",
       "         [  42.],\n",
       "         [  38.],\n",
       "         [  29.],\n",
       "         [ 368.],\n",
       "         [ 132.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 190.],\n",
       "         [ 130.],\n",
       "         [ 130.],\n",
       "         [ 163.],\n",
       "         [ 196.],\n",
       "         [  46.],\n",
       "         [  74.],\n",
       "         [  85.],\n",
       "         [ 148.],\n",
       "         [  41.],\n",
       "         [ 120.],\n",
       "         [ 328.],\n",
       "         [ 181.],\n",
       "         [ 302.],\n",
       "         [  37.],\n",
       "         [  43.],\n",
       "         [  37.],\n",
       "         [ 441.],\n",
       "         [ 501.],\n",
       "         [ 109.],\n",
       "         [  30.],\n",
       "         [  64.],\n",
       "         [ 113.],\n",
       "         [  61.],\n",
       "         [  40.],\n",
       "         [  72.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 669.],\n",
       "         [  28.],\n",
       "         [  27.],\n",
       "         [  28.],\n",
       "         [ 595.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  28.],\n",
       "         [ 938.],\n",
       "         [  27.],\n",
       "         [  30.],\n",
       "         [  41.],\n",
       "         [  27.],\n",
       "         [  35.],\n",
       "         [ 180.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  75.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  34.],\n",
       "         [  34.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [ 148.],\n",
       "         [  27.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 307.],\n",
       "         [  28.],\n",
       "         [  27.],\n",
       "         [  35.],\n",
       "         [ 176.],\n",
       "         [  29.],\n",
       "         [  28.],\n",
       "         [  50.],\n",
       "         [ 175.],\n",
       "         [  27.],\n",
       "         [  33.],\n",
       "         [  40.],\n",
       "         [  29.],\n",
       "         [  85.],\n",
       "         [  63.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  49.],\n",
       "         [  47.],\n",
       "         [  35.],\n",
       "         [  52.],\n",
       "         [  27.],\n",
       "         [  29.],\n",
       "         [  27.],\n",
       "         [ 100.],\n",
       "         [  28.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 130.],\n",
       "         [  28.],\n",
       "         [  31.],\n",
       "         [  32.],\n",
       "         [  63.],\n",
       "         [  30.],\n",
       "         [  27.],\n",
       "         [  28.],\n",
       "         [ 129.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  66.],\n",
       "         [  28.],\n",
       "         [  28.],\n",
       "         [  68.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  27.],\n",
       "         [  58.],\n",
       "         [  97.],\n",
       "         [  32.],\n",
       "         [  27.],\n",
       "         [  30.],\n",
       "         [  65.],\n",
       "         [  57.],\n",
       "         [  46.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [2170.],\n",
       "         [  54.],\n",
       "         [ 142.],\n",
       "         [ 299.],\n",
       "         [ 328.],\n",
       "         [  39.],\n",
       "         [  57.],\n",
       "         [  49.],\n",
       "         [ 219.],\n",
       "         [  50.],\n",
       "         [ 113.],\n",
       "         [1131.],\n",
       "         [ 175.],\n",
       "         [1853.],\n",
       "         [ 298.],\n",
       "         [  42.],\n",
       "         [  33.],\n",
       "         [ 318.],\n",
       "         [ 428.],\n",
       "         [ 131.],\n",
       "         [ 168.],\n",
       "         [ 133.],\n",
       "         [  31.],\n",
       "         [  55.],\n",
       "         [  50.],\n",
       "         [ 105.]],\n",
       "\n",
       "        [[  27.],\n",
       "         [ 887.],\n",
       "         [  31.],\n",
       "         [  29.],\n",
       "         [  29.],\n",
       "         [ 400.],\n",
       "         [  27.],\n",
       "         [  28.],\n",
       "         [  70.],\n",
       "         [ 391.],\n",
       "         [  29.],\n",
       "         [  29.],\n",
       "         [ 150.],\n",
       "         [  62.],\n",
       "         [  31.],\n",
       "         [ 137.],\n",
       "         [  29.],\n",
       "         [  27.],\n",
       "         [  59.],\n",
       "         [  31.],\n",
       "         [  31.],\n",
       "         [ 100.],\n",
       "         [  29.],\n",
       "         [  30.],\n",
       "         [  28.],\n",
       "         [ 174.],\n",
       "         [  72.]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.sum(2, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdim = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to generate some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "ilyasid.\n",
      "prelay.\n",
      "ocin.\n",
      "fairritoper.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix1, ix2 = 0, 0  # Start with the beginning token ('.', '.')\n",
    "\n",
    "  while True:\n",
    "    p = P[ix1, ix2]  # Get the probability distribution for the next character\n",
    "    ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix3])  # Convert index back to character\n",
    "    \n",
    "    if ix3 == 0:  # Stop when the end token '.' is generated\n",
    "      break\n",
    "\n",
    "    # Shift indices: move (ix2 -> ix1) and (ix3 -> ix2) for the next iteration\n",
    "    ix1, ix2 = ix2, ix3\n",
    "\n",
    "  print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the normalised negative loglikelihood and compare it to the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-410414.9688)\n",
      "nll=tensor(410414.9688)\n",
      "2.092747449874878\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]      \n",
    "    prob = P[ix1, ix2, ix3]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    n += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model has improved w.r.t the bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "---------------------\n",
    "We now create the same model using a one layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set of trigrams: input (ch1, ch2) and output ch3\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]    \n",
    "    xs.append([ix1, ix2])\n",
    "    ys.append(ix3)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num_examples = xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two indices into a single index for a bigram.\n",
    "bigram_idx = xs[:, 0] * 27 + xs[:, 1]  # shape: (N,)\n",
    "\n",
    "# One-hot encode the bigram indices into a 729-dimensional vector.\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(bigram_idx, num_classes=27*27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27*27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1307287216186523\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(1):\n",
    "  \n",
    "  # forward pass\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num_examples), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only do so with the trigram model. We will begin by shuffling the list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set of trigrams: input (ch1, ch2) and output ch3\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]    \n",
    "    xs.append([ix1, ix2])\n",
    "    ys.append(ix3)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num_examples = xs.shape[0]\n",
    "# Combine the two indices into a single index for a bigram.\n",
    "bigram_idx = xs[:, 0] * 27 + xs[:, 1]  # shape: (N,)\n",
    "\n",
    "# One-hot encode the bigram indices into a 729-dimensional vector.\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(bigram_idx, num_classes=27*27).float()\n",
    "\n",
    "# calculationg index for splits\n",
    "a = int(num_examples*0.8)\n",
    "b = int(num_examples*0.9)\n",
    "\n",
    "# creating training dev and test set after encoding has been done\n",
    "xs_tr, xs_dev, xs_te = xenc[:a], xenc[a:b], xenc[b:]\n",
    "ys_tr, ys_dev, ys_te = ys[:a], ys[a:b], ys[b:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27*27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1671576499938965\n",
      "2.166998863220215\n",
      "2.1668405532836914\n",
      "2.166682720184326\n",
      "2.1665258407592773\n",
      "2.1663694381713867\n",
      "2.1662135124206543\n",
      "2.166058301925659\n",
      "2.1659035682678223\n",
      "2.1657493114471436\n",
      "2.165595531463623\n",
      "2.165442705154419\n",
      "2.165290117263794\n",
      "2.1651382446289062\n",
      "2.1649866104125977\n",
      "2.1648359298706055\n",
      "2.1646854877471924\n",
      "2.1645357608795166\n",
      "2.164386510848999\n",
      "2.1642377376556396\n",
      "2.1640896797180176\n",
      "2.1639420986175537\n",
      "2.163794994354248\n",
      "2.1636481285095215\n",
      "2.1635022163391113\n",
      "2.1633565425872803\n",
      "2.1632115840911865\n",
      "2.163067102432251\n",
      "2.1629230976104736\n",
      "2.1627795696258545\n",
      "2.1626367568969727\n",
      "2.162493944168091\n",
      "2.1623520851135254\n",
      "2.162210702896118\n",
      "2.162069320678711\n",
      "2.16192889213562\n",
      "2.1617889404296875\n",
      "2.161649465560913\n",
      "2.161510467529297\n",
      "2.1613717079162598\n",
      "2.161233901977539\n",
      "2.1610960960388184\n",
      "2.160959005355835\n",
      "2.1608221530914307\n",
      "2.1606860160827637\n",
      "2.160550594329834\n",
      "2.1604151725769043\n",
      "2.160280227661133\n",
      "2.1601459980010986\n",
      "2.1600120067596436\n",
      "2.159878730773926\n",
      "2.159745454788208\n",
      "2.1596131324768066\n",
      "2.1594810485839844\n",
      "2.159349203109741\n",
      "2.1592180728912354\n",
      "2.1590874195098877\n",
      "2.158957004547119\n",
      "2.158827304840088\n",
      "2.158698081970215\n",
      "2.158568859100342\n",
      "2.158440351486206\n",
      "2.1583118438720703\n",
      "2.158184289932251\n",
      "2.1580569744110107\n",
      "2.1579301357269287\n",
      "2.157803535461426\n",
      "2.15767765045166\n",
      "2.1575520038604736\n",
      "2.1574268341064453\n",
      "2.157302141189575\n",
      "2.1571779251098633\n",
      "2.1570537090301514\n",
      "2.1569302082061768\n",
      "2.156806707382202\n",
      "2.156684398651123\n",
      "2.1565613746643066\n",
      "2.1564395427703857\n",
      "2.156318426132202\n",
      "2.1561970710754395\n",
      "2.156076192855835\n",
      "2.1559557914733887\n",
      "2.1558358669281006\n",
      "2.1557159423828125\n",
      "2.15559720993042\n",
      "2.1554782390594482\n",
      "2.1553595066070557\n",
      "2.1552417278289795\n",
      "2.155123710632324\n",
      "2.1550064086914062\n",
      "2.1548895835876465\n",
      "2.154772996902466\n",
      "2.1546566486358643\n",
      "2.154541015625\n",
      "2.1544253826141357\n",
      "2.1543099880218506\n",
      "2.154195547103882\n",
      "2.154081106185913\n",
      "2.1539666652679443\n",
      "2.153853416442871\n",
      "2.1537399291992188\n",
      "2.1536271572113037\n",
      "2.1535143852233887\n",
      "2.153402090072632\n",
      "2.153290271759033\n",
      "2.1531786918640137\n",
      "2.1530673503875732\n",
      "2.15295672416687\n",
      "2.152846097946167\n",
      "2.152735948562622\n",
      "2.152625799179077\n",
      "2.1525163650512695\n",
      "2.152407169342041\n",
      "2.1522984504699707\n",
      "2.1521899700164795\n",
      "2.1520819664001465\n",
      "2.1519742012023926\n",
      "2.1518661975860596\n",
      "2.151759386062622\n",
      "2.1516523361206055\n",
      "2.151545763015747\n",
      "2.151439905166626\n",
      "2.151333808898926\n",
      "2.151228427886963\n",
      "2.151123046875\n",
      "2.151017904281616\n",
      "2.1509134769439697\n",
      "2.1508092880249023\n",
      "2.150705099105835\n",
      "2.150601625442505\n",
      "2.150498151779175\n",
      "2.150394916534424\n",
      "2.150292158126831\n",
      "2.1501898765563965\n",
      "2.150087833404541\n",
      "2.1499860286712646\n",
      "2.149883985519409\n",
      "2.149782657623291\n",
      "2.149681806564331\n",
      "2.1495814323425293\n",
      "2.1494810581207275\n",
      "2.149381160736084\n",
      "2.1492815017700195\n",
      "2.149181842803955\n",
      "2.1490824222564697\n",
      "2.1489837169647217\n",
      "2.1488852500915527\n",
      "2.148787260055542\n",
      "2.148688793182373\n",
      "2.1485912799835205\n",
      "2.148493766784668\n",
      "2.1483967304229736\n",
      "2.1482994556427\n",
      "2.148203134536743\n",
      "2.148106813430786\n",
      "2.1480109691619873\n",
      "2.1479151248931885\n",
      "2.1478192806243896\n",
      "2.147724151611328\n",
      "2.147629499435425\n",
      "2.1475346088409424\n",
      "2.1474404335021973\n",
      "2.147346019744873\n",
      "2.1472525596618652\n",
      "2.1471588611602783\n",
      "2.1470656394958496\n",
      "2.146972179412842\n",
      "2.1468796730041504\n",
      "2.14678692817688\n",
      "2.1466946601867676\n",
      "2.1466026306152344\n",
      "2.1465110778808594\n",
      "2.1464195251464844\n",
      "2.1463284492492676\n",
      "2.146237373352051\n",
      "2.146146535873413\n",
      "2.1460561752319336\n",
      "2.145966053009033\n",
      "2.145875930786133\n",
      "2.1457862854003906\n",
      "2.1456968784332275\n",
      "2.1456074714660645\n",
      "2.1455185413360596\n",
      "2.145429849624634\n",
      "2.145341396331787\n",
      "2.1452531814575195\n",
      "2.145165205001831\n",
      "2.1450774669647217\n",
      "2.1449899673461914\n",
      "2.144902467727661\n",
      "2.144815683364868\n",
      "2.144728660583496\n",
      "2.1446425914764404\n",
      "2.1445560455322266\n",
      "2.144469976425171\n",
      "2.1443839073181152\n",
      "2.144298791885376\n",
      "2.1442134380340576\n",
      "2.1441280841827393\n",
      "2.144043207168579\n",
      "2.143958568572998\n",
      "2.143873929977417\n",
      "2.143789768218994\n",
      "2.1437056064605713\n",
      "2.1436219215393066\n",
      "2.143538475036621\n",
      "2.1434550285339355\n",
      "2.143372058868408\n",
      "2.143289089202881\n",
      "2.1432063579559326\n",
      "2.1431243419647217\n",
      "2.1430418491363525\n",
      "2.1429600715637207\n",
      "2.142878293991089\n",
      "2.1427969932556152\n",
      "2.1427152156829834\n",
      "2.142634153366089\n",
      "2.1425530910491943\n",
      "2.142472505569458\n",
      "2.142392158508301\n",
      "2.1423118114471436\n",
      "2.1422319412231445\n",
      "2.1421520709991455\n",
      "2.1420722007751465\n",
      "2.1419928073883057\n",
      "2.141913652420044\n",
      "2.1418349742889404\n",
      "2.1417558193206787\n",
      "2.141677141189575\n",
      "2.14159893989563\n",
      "2.1415207386016846\n",
      "2.1414427757263184\n",
      "2.1413650512695312\n",
      "2.1412875652313232\n",
      "2.1412100791931152\n",
      "2.1411328315734863\n",
      "2.1410560607910156\n",
      "2.1409788131713867\n",
      "2.140902519226074\n",
      "2.1408262252807617\n",
      "2.140749931335449\n",
      "2.140673875808716\n",
      "2.1405978202819824\n",
      "2.1405224800109863\n",
      "2.140446901321411\n",
      "2.1403720378875732\n",
      "2.1402969360351562\n",
      "2.1402220726013184\n",
      "2.1401474475860596\n",
      "2.14007306098938\n",
      "2.1399986743927\n",
      "2.1399245262145996\n",
      "2.139850616455078\n",
      "2.139777183532715\n",
      "2.1397035121917725\n",
      "2.1396303176879883\n",
      "2.139556884765625\n",
      "2.139484167098999\n",
      "2.139410972595215\n",
      "2.139338731765747\n",
      "2.1392664909362793\n",
      "2.1391940116882324\n",
      "2.139122247695923\n",
      "2.139050245285034\n",
      "2.1389782428741455\n",
      "2.1389071941375732\n",
      "2.1388354301452637\n",
      "2.1387643814086914\n",
      "2.138693332672119\n",
      "2.138622760772705\n",
      "2.138551950454712\n",
      "2.138481616973877\n",
      "2.138411045074463\n",
      "2.1383414268493652\n",
      "2.1382713317871094\n",
      "2.1382017135620117\n",
      "2.138131618499756\n",
      "2.1380622386932373\n",
      "2.137993335723877\n",
      "2.1379241943359375\n",
      "2.137855291366577\n",
      "2.137786388397217\n",
      "2.1377179622650146\n",
      "2.1376495361328125\n",
      "2.1375813484191895\n",
      "2.1375131607055664\n",
      "2.1374456882476807\n",
      "2.1373775005340576\n",
      "2.137310028076172\n",
      "2.137242555618286\n",
      "2.1371753215789795\n",
      "2.137108087539673\n",
      "2.1370413303375244\n",
      "2.136974573135376\n",
      "2.1369078159332275\n",
      "2.136841297149658\n",
      "2.136775016784668\n",
      "2.136708974838257\n",
      "2.136643171310425\n",
      "2.1365773677825928\n",
      "2.1365115642547607\n",
      "2.136446237564087\n",
      "2.136380672454834\n",
      "2.1363155841827393\n",
      "2.1362507343292236\n",
      "2.13618540763855\n",
      "2.1361207962036133\n",
      "2.1360561847686768\n",
      "2.1359920501708984\n",
      "2.135927677154541\n",
      "2.1358635425567627\n",
      "2.1357996463775635\n",
      "2.1357357501983643\n",
      "2.135671854019165\n",
      "2.135608434677124\n",
      "2.135545492172241\n",
      "2.1354820728302\n",
      "2.1354191303253174\n",
      "2.1353559494018555\n",
      "2.1352932453155518\n",
      "2.135230779647827\n",
      "2.1351680755615234\n",
      "2.135105609893799\n",
      "2.1350436210632324\n",
      "2.134981393814087\n",
      "2.1349196434020996\n",
      "2.1348578929901123\n",
      "2.134795904159546\n",
      "2.134734630584717\n",
      "2.1346733570098877\n",
      "2.1346120834350586\n",
      "2.1345512866973877\n",
      "2.1344902515411377\n",
      "2.134429454803467\n",
      "2.134368896484375\n",
      "2.134308338165283\n",
      "2.1342482566833496\n",
      "2.134188175201416\n",
      "2.1341278553009033\n",
      "2.134068012237549\n",
      "2.1340079307556152\n",
      "2.133948564529419\n",
      "2.1338887214660645\n",
      "2.1338295936584473\n",
      "2.133770227432251\n",
      "2.1337108612060547\n",
      "2.1336517333984375\n",
      "2.1335933208465576\n",
      "2.1335346698760986\n",
      "2.1334760189056396\n",
      "2.1334176063537598\n",
      "2.133358955383301\n",
      "2.133301019668579\n",
      "2.133242607116699\n",
      "2.1331849098205566\n",
      "2.133127212524414\n",
      "2.1330692768096924\n",
      "2.133012056350708\n",
      "2.1329543590545654\n",
      "2.13289737701416\n",
      "2.132840156555176\n",
      "2.1327831745147705\n",
      "2.1327261924743652\n",
      "2.13266921043396\n",
      "2.132612466812134\n",
      "2.1325559616088867\n",
      "2.132499933242798\n",
      "2.132443428039551\n",
      "2.132387161254883\n",
      "2.132331609725952\n",
      "2.132275342941284\n",
      "2.1322195529937744\n",
      "2.1321637630462646\n",
      "2.132108449935913\n",
      "2.1320528984069824\n",
      "2.13199782371521\n",
      "2.1319425106048584\n",
      "2.131887435913086\n",
      "2.1318325996398926\n",
      "2.131777763366699\n",
      "2.131722927093506\n",
      "2.1316683292388916\n",
      "2.1316139698028564\n",
      "2.1315596103668213\n",
      "2.131505250930786\n",
      "2.131451368331909\n",
      "2.1313977241516113\n",
      "2.1313436031341553\n",
      "2.1312897205352783\n",
      "2.1312360763549805\n",
      "2.1311826705932617\n",
      "2.131129026412964\n",
      "2.131075859069824\n",
      "2.1310226917266846\n",
      "2.130969524383545\n",
      "2.1309168338775635\n",
      "2.130863666534424\n",
      "2.1308112144470215\n",
      "2.13075852394104\n",
      "2.130706310272217\n",
      "2.1306538581848145\n",
      "2.130601644515991\n",
      "2.130549430847168\n",
      "2.1304972171783447\n",
      "2.1304452419281006\n",
      "2.1303937435150146\n",
      "2.1303417682647705\n",
      "2.1302905082702637\n",
      "2.1302385330200195\n",
      "2.1301872730255127\n",
      "2.130136251449585\n",
      "2.130084991455078\n",
      "2.1300337314605713\n",
      "2.1299829483032227\n",
      "2.129932403564453\n",
      "2.1298816204071045\n",
      "2.1298305988311768\n",
      "2.1297802925109863\n",
      "2.129729747772217\n",
      "2.1296796798706055\n",
      "2.129629611968994\n",
      "2.1295790672302246\n",
      "2.1295292377471924\n",
      "2.12947940826416\n",
      "2.129429578781128\n",
      "2.1293795108795166\n",
      "2.1293301582336426\n",
      "2.1292808055877686\n",
      "2.1292314529418945\n",
      "2.1291821002960205\n",
      "2.1291329860687256\n",
      "2.1290838718414307\n",
      "2.129034996032715\n",
      "2.128986120223999\n",
      "2.128937244415283\n",
      "2.1288886070251465\n",
      "2.1288399696350098\n",
      "2.1287918090820312\n",
      "2.1287431716918945\n",
      "2.128695249557495\n",
      "2.1286470890045166\n",
      "2.128598690032959\n",
      "2.1285507678985596\n",
      "2.1285030841827393\n",
      "2.12845516204834\n",
      "2.1284077167510986\n",
      "2.1283600330352783\n",
      "2.128312587738037\n",
      "2.128265142440796\n",
      "2.128218173980713\n",
      "2.128170967102051\n",
      "2.1281235218048096\n",
      "2.1280767917633057\n",
      "2.1280295848846436\n",
      "2.1279828548431396\n",
      "2.1279361248016357\n",
      "2.127889633178711\n",
      "2.127842903137207\n",
      "2.1277966499328613\n",
      "2.1277503967285156\n",
      "2.127703905105591\n",
      "2.127657890319824\n",
      "2.1276118755340576\n",
      "2.127565860748291\n",
      "2.1275198459625244\n",
      "2.127474546432495\n",
      "2.1274287700653076\n",
      "2.12738299369812\n",
      "2.1273372173309326\n",
      "2.1272919178009033\n",
      "2.127246618270874\n",
      "2.1272013187408447\n",
      "2.1271562576293945\n",
      "2.1271114349365234\n",
      "2.1270663738250732\n",
      "2.127021551132202\n",
      "2.126976728439331\n",
      "2.12693190574646\n",
      "2.126887321472168\n",
      "2.126842975616455\n",
      "2.126798391342163\n",
      "2.126753807067871\n",
      "2.126709461212158\n",
      "2.1266655921936035\n",
      "2.126621723175049\n",
      "2.126577615737915\n",
      "2.1265335083007812\n",
      "2.1264898777008057\n",
      "2.126446008682251\n",
      "2.1264023780822754\n",
      "2.1263585090637207\n",
      "2.1263153553009033\n",
      "2.126271963119507\n",
      "2.1262285709381104\n",
      "2.126185417175293\n",
      "2.1261420249938965\n",
      "2.1260993480682373\n",
      "2.12605619430542\n",
      "2.1260132789611816\n",
      "2.1259706020355225\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(500):\n",
    "  \n",
    "  # forward pass\n",
    "  logits = xs_tr @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(a), ys_tr].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.1583569049835205\n",
      "Test loss: 2.14095139503479\n"
     ]
    }
   ],
   "source": [
    "def evaluate(xenc_eval, ys_eval, W):\n",
    "    num_eval = xenc_eval.shape[0]\n",
    "    logits_eval = xenc_eval @ W\n",
    "    counts_eval = logits_eval.exp()\n",
    "    probs_eval  = counts_eval / counts_eval.sum(1, keepdims=True)\n",
    "    loss_eval = -probs_eval[torch.arange(num_eval), ys_eval].log().mean() + 0.01 * (W**2).mean()\n",
    "    return loss_eval.item()\n",
    "\n",
    "dev_loss  = evaluate(xs_dev, ys_dev, W)\n",
    "test_loss = evaluate(xs_te, ys_te, W)\n",
    "print(\"Dev loss:\", dev_loss)\n",
    "print(\"Test loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will rewrite almost alll the code so that variables are reinitialised in the right moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg: 0.001 | Train loss: 2.1533 | Dev loss: 2.1827\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# Shuffle the words list so that your splits are random.\n",
    "random.shuffle(words)\n",
    "\n",
    "# Build the trigram dataset: inputs (bigram: ch1,ch2) and target (ch3)\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)  # shape: (N, 2)\n",
    "ys = torch.tensor(ys)  # shape: (N,)\n",
    "num_examples = xs.shape[0]\n",
    "\n",
    "# Combine the two indices into a single unique index for each bigram.\n",
    "bigram_idx = xs[:, 0] * 27 + xs[:, 1]  # shape: (N,)\n",
    "\n",
    "# One-hot encode the bigram indices into a vector of size 27*27 = 729.\n",
    "xenc = F.one_hot(bigram_idx, num_classes=27*27).float()\n",
    "\n",
    "# Split the data into training (80%), dev (10%), and test (10%)\n",
    "a = int(num_examples * 0.8)\n",
    "b = int(num_examples * 0.9)\n",
    "xs_tr, xs_dev, xs_te = xenc[:a], xenc[a:b], xenc[b:]\n",
    "ys_tr, ys_dev, ys_te = ys[:a], ys[a:b], ys[b:]\n",
    "\n",
    "# List of candidate regularization strengths (smoothing factors) to try.\n",
    "reg_strengths = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "results = {}  # To store final train and dev losses for each regularization value\n",
    "\n",
    "for reg in reg_strengths:\n",
    "    # Reinitialize weights for each experiment.\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((27*27, 27), generator=g, requires_grad=True)\n",
    "    \n",
    "    # Train the model on the training set for a fixed number of iterations.\n",
    "    num_train = xs_tr.shape[0]\n",
    "    learning_rate = 50  # you can adjust this if needed\n",
    "    for k in range(500):\n",
    "        # Forward pass on training set.\n",
    "        logits = xs_tr @ W      # (num_train, 27)\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Compute the loss:\n",
    "        # - Negative log-likelihood of the correct target (first term).\n",
    "        # - Plus the L2 regularization on W (weighted by 'reg').\n",
    "        loss_train = -probs[torch.arange(num_train), ys_tr].log().mean() + reg * (W**2).mean()\n",
    "        \n",
    "        # Backward pass and weight update.\n",
    "        W.grad = None\n",
    "        loss_train.backward()\n",
    "        W.data += -learning_rate * W.grad  # simple gradient descent update\n",
    "    \n",
    "    # After training, evaluate on the dev set.\n",
    "    num_dev = xs_dev.shape[0]\n",
    "    logits_dev = xs_dev @ W\n",
    "    counts_dev = logits_dev.exp()\n",
    "    probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "    loss_dev = -probs_dev[torch.arange(num_dev), ys_dev].log().mean() + reg * (W**2).mean()\n",
    "    \n",
    "    results[reg] = (loss_train.item(), loss_dev.item())\n",
    "    print(f\"Reg: {reg:>5} | Train loss: {loss_train.item():.4f} | Dev loss: {loss_dev.item():.4f}\")\n",
    "\n",
    "# Choose the best regularization strength based on dev loss.\n",
    "best_reg = min(results, key=lambda r: results[r][1])\n",
    "print(\"\\nBest regularization strength based on dev set:\", best_reg)\n",
    "\n",
    "# Now, for the best setting, re-train (or reuse the trained model) and evaluate on the test set.\n",
    "# Here, for clarity, we reinitialize and train with the best regularization setting.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)\n",
    "num_train = xs_tr.shape[0]\n",
    "for k in range(500):\n",
    "    logits = xs_tr @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss_train = -probs[torch.arange(num_train), ys_tr].log().mean() + best_reg * (W**2).mean()\n",
    "    W.grad = None\n",
    "    loss_train.backward()\n",
    "    W.data += -learning_rate * W.grad\n",
    "\n",
    "# Evaluate on the test set.\n",
    "num_test = xs_te.shape[0]\n",
    "logits_test = xs_te @ W\n",
    "counts_test = logits_test.exp()\n",
    "probs_test = counts_test / counts_test.sum(1, keepdims=True)\n",
    "loss_test = -probs_test[torch.arange(num_test), ys_te].log().mean() + best_reg * (W**2).mean()\n",
    "print(f\"Test loss with best regularization ({best_reg}): {loss_test.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
